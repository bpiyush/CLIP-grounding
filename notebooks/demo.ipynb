{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f3c8c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b64dfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tried to automate, but didn't work :(\n",
    "# import os\n",
    "\n",
    "# repo_root = os.path.join(os.path.dirname(os.getcwd()))\n",
    "# !export PYTHONPATH={repo_root}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "237ed6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from os.path import join\n",
    "\n",
    "try:\n",
    "    from clip_grounding.utils.paths import REPO_PATH\n",
    "    sys.path.append(join(REPO_PATH, \"CLIP_explainability/Transformer-MM-Explainability/\"))\n",
    "except ImportError:\n",
    "    print(\"Cannot import 'clip_grounding.utils.paths.REPO_PATH'\")\n",
    "    print(\"To fix: use 'export PYTHONPATH=$PWD'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f390612e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.patches import Patch\n",
    "import CLIP.clip as clip\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "from natsort import natsorted\n",
    "\n",
    "from clip_grounding.utils.paths import REPO_PATH\n",
    "from clip_grounding.utils.io import load_json\n",
    "from clip_grounding.utils.visualize import set_latex_fonts, show_grid_of_images\n",
    "from clip_grounding.utils.image import pad_to_square\n",
    "from clip_grounding.datasets.png_utils import show_images_and_caption\n",
    "from clip_grounding.datasets.png import (\n",
    "    PNG,\n",
    "    visualize_item,\n",
    "    overlay_segmask_on_image,\n",
    "    overlay_relevance_map_on_image,\n",
    "    get_text_colors,\n",
    ")\n",
    "from clip_grounding.evaluation.clip_on_png import (\n",
    "    process_entry_image_to_text,\n",
    "    process_entry_text_to_image,\n",
    "    interpret_and_generate,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f22dd2",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b05e530",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PNG(dataset_root=join(REPO_PATH, \"data/panoptic_narrative_grounding\"), split=\"val2017\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb41079d",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d6e6961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load CLIP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636017bf",
   "metadata": {},
   "source": [
    "### Load and visualize a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5645b9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_entry_text_to_image(entry, pad_images=True, figsize=(18, 5)):\n",
    "    test_img, test_texts, orig_image = process_entry_text_to_image(entry, unimodal=False)\n",
    "    outputs = interpret_and_generate(model, test_img, test_texts, orig_image, return_outputs=True, show=False)\n",
    "    relevance_map = outputs[0][\"image_relevance\"]\n",
    "    \n",
    "    image_with_mask = overlay_segmask_on_image(entry[\"image\"], entry[\"image_mask\"])\n",
    "    if pad_images:\n",
    "        image_with_mask = pad_to_square(image_with_mask)\n",
    "    \n",
    "    image_with_relevance_map = overlay_relevance_map_on_image(entry[\"image\"], relevance_map)\n",
    "    if pad_images:\n",
    "        image_with_relevance_map = pad_to_square(image_with_relevance_map)\n",
    "    \n",
    "    text_colors = get_text_colors(entry[\"text\"], entry[\"text_mask\"])\n",
    "    \n",
    "    show_images_and_caption(\n",
    "        [image_with_mask, image_with_relevance_map],\n",
    "        entry[\"text\"], text_colors, figsize=figsize,\n",
    "        image_xlabels=[\"Ground truth segmentation\", \"Predicted relevance map\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "743c5e65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.5.3'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f806f8ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> 640 426\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.3) :-1: error: (-5:Bad argument) in function 'resize'\n> Overload resolution failed:\n>  - src data type = 23 is not supported\n>  - Expected Ptr<cv::UMat> for argument 'src'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b75de15d253c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"full_caption\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mvisualize_entry_text_to_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_images\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m19\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-cadeb230847e>\u001b[0m in \u001b[0;36mvisualize_entry_text_to_image\u001b[0;34m(entry, pad_images, figsize)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mimage_with_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_to_square\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_with_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mimage_with_relevance_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moverlay_relevance_map_on_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelevance_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpad_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mimage_with_relevance_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_to_square\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_with_relevance_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/jaar4/ie/CLIP-grounding/clip_grounding/datasets/png.py\u001b[0m in \u001b[0;36moverlay_relevance_map_on_image\u001b[0;34m(image, heatmap)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;31m# resize the heatmap to image size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m     \u001b[0mheatmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m     \u001b[0mheatmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mheatmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0mheatmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplyColorMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLORMAP_JET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.5.3) :-1: error: (-5:Bad argument) in function 'resize'\n> Overload resolution failed:\n>  - src data type = 23 is not supported\n>  - Expected Ptr<cv::UMat> for argument 'src'\n"
     ]
    }
   ],
   "source": [
    "idx = 100\n",
    "instance = dataset[idx]\n",
    "\n",
    "instance_dir = join(REPO_PATH, \"figures\", f\"instance-{idx}\")\n",
    "os.makedirs(instance_dir, exist_ok=True)\n",
    "\n",
    "for i, entry in enumerate(instance):\n",
    "    del entry[\"full_caption\"]\n",
    "\n",
    "    visualize_entry_text_to_image(entry, pad_images=False, figsize=(19, 4))\n",
    "    \n",
    "    save_path = instance_dir\n",
    "    plt.savefig(join(instance_dir, f\"viz-{i}.png\"), bbox_inches=\"tight\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc73b10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_save_gif(filenames, save_path, **kwargs):\n",
    "    import imageio\n",
    "    images = []\n",
    "    for filename in filenames:\n",
    "        images.append(imageio.imread(filename))\n",
    "    imageio.mimsave(save_path, images, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3966a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = natsorted(glob(join(instance_dir, \"viz-*.png\")))\n",
    "# save_path = join(instance_dir, \"together.gif\")\n",
    "save_path = join(REPO_PATH, \"media\", \"sample.gif\")\n",
    "\n",
    "create_and_save_gif(filenames, save_path, duration=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de231eed",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Debugging code for a single entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009f51a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 100\n",
    "instance = dataset[idx]\n",
    "entry = instance[1]\n",
    "del entry[\"full_caption\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468ef802",
   "metadata": {},
   "outputs": [],
   "source": [
    "entry.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb58467d",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_item(**entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23181366",
   "metadata": {},
   "source": [
    "### Visualize everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be04b7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img, test_texts, orig_image = process_entry_text_to_image(entry, unimodal=False)\n",
    "outputs = interpret_and_generate(model, test_img, test_texts, orig_image, return_outputs=True, show=False)\n",
    "relevance_map = outputs[0][\"image_relevance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08844ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_with_mask = overlay_segmask_on_image(entry[\"image\"], entry[\"image_mask\"])\n",
    "image_with_mask = pad_to_square(image_with_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563eb049",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_with_mask.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c0ebd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_map.min(), relevance_map.max(), relevance_map.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c708bb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_with_relevance_map = overlay_relevance_map_on_image(entry[\"image\"], relevance_map)\n",
    "image_with_relevance_map = pad_to_square(image_with_relevance_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6582856",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_colors = get_text_colors(entry[\"text\"], entry[\"text_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46546a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images_and_caption([image_with_mask, image_with_relevance_map], entry[\"text\"], text_colors, figsize=(18, 5))\n",
    "plt.savefig(\"../figures/viz-1.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf8d46a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
