{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/hila-chefer/Transformer-MM-Explainability/blob/main/CLIP_explainability.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HiplefhuIUDd"
   },
   "source": [
    "# **CLIP Explainability**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already cloned: skipping..\n",
      "Requirement already satisfied: einops in /home/danilo/miniconda3/lib/python3.7/site-packages (0.4.1)\n",
      "Requirement already satisfied: ftfy in /home/danilo/miniconda3/lib/python3.7/site-packages (6.1.1)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /home/danilo/miniconda3/lib/python3.7/site-packages (from ftfy) (0.2.5)\n",
      "Requirement already satisfied: captum in /home/danilo/miniconda3/lib/python3.7/site-packages (0.5.0)\n",
      "Requirement already satisfied: numpy in /home/danilo/miniconda3/lib/python3.7/site-packages (from captum) (1.21.2)\n",
      "Requirement already satisfied: matplotlib in /home/danilo/miniconda3/lib/python3.7/site-packages (from captum) (3.4.3)\n",
      "Requirement already satisfied: torch>=1.6 in /home/danilo/miniconda3/lib/python3.7/site-packages (from captum) (1.10.0)\n",
      "Requirement already satisfied: typing_extensions in /home/danilo/miniconda3/lib/python3.7/site-packages (from torch>=1.6->captum) (3.10.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/danilo/miniconda3/lib/python3.7/site-packages (from matplotlib->captum) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/danilo/miniconda3/lib/python3.7/site-packages (from matplotlib->captum) (3.0.6)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/danilo/miniconda3/lib/python3.7/site-packages (from matplotlib->captum) (8.4.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/danilo/miniconda3/lib/python3.7/site-packages (from matplotlib->captum) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/danilo/miniconda3/lib/python3.7/site-packages (from matplotlib->captum) (1.3.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/danilo/miniconda3/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib->captum) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(f'./Transformer-MM-Explainability'):\n",
    "    !git clone https://github.com/hila-chefer/Transformer-MM-Explainability\n",
    "else:\n",
    "    print(\"Already cloned: skipping..\")\n",
    "\n",
    "os.chdir(f'./Transformer-MM-Explainability')\n",
    "\n",
    "!pip install einops\n",
    "!pip install ftfy\n",
    "!pip install captum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8sl0DTeHuKx"
   },
   "source": [
    "# **CLIP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "or8UETbZAYY3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import CLIP.clip as clip\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from captum.attr import visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fWKGyu2YAeSV"
   },
   "outputs": [],
   "source": [
    "from utils import interpret, show_image_relevance, show_heatmap_on_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "7YYjztv3Nn9V"
   },
   "outputs": [],
   "source": [
    "clip.clip._MODELS = {\n",
    "    \"ViT-B/32\": \"https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\",\n",
    "    \"ViT-B/16\": \"https://openaipublic.azureedge.net/clip/models/5806e77cd80f8b59890b7e101eabd078d9fb84e6937f9e85e4ecb61988df416f/ViT-B-16.pt\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pns9sG9eAhho",
    "outputId": "30eb7b33-b089-4051-d0b9-ab261d07dc39"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danilo/miniconda3/lib/python3.7/site-packages/torchvision/transforms/transforms.py:288: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "4hIrp94ktMyc"
   },
   "outputs": [],
   "source": [
    "class color:\n",
    "    PURPLE = '\\033[95m'\n",
    "    CYAN = '\\033[96m'\n",
    "    DARKCYAN = '\\033[36m'\n",
    "    BLUE = '\\033[94m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "id": "WoeuTHtyBq7Q",
    "outputId": "ab3cd50f-4b72-435a-a131-9b08e8859964"
   },
   "outputs": [],
   "source": [
    "img_path = \"CLIP/glasses.png\"\n",
    "img = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n",
    "texts = [\"a man with eyeglasses\"]\n",
    "text = clip.tokenize(texts).to(device)\n",
    "\n",
    "R_text, R_image = interpret(model=model, image=img, texts=text, device=device)\n",
    "batch_size = text.shape[0]\n",
    "for i in range(batch_size):\n",
    "    show_heatmap_on_text(texts[i], text[i], R_text[i])\n",
    "    show_image_relevance(R_image[i], img, orig_image=Image.open(img_path))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "CLIP-explainability.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
